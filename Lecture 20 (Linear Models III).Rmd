---
title: "Lecture 20 (Linear Models III)"
author: "CJ Snyder"
date: "11/14/2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(readxl)
library(viridis)
library(patchwork)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))

library(modelr)
library(mgcv)

library(p8105.datasets)

set.seed(1)
```

# **Bootstrapping**
```{r}
n_samp = 250

sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

sim_df_nonconst = sim_df_const %>% 
  mutate(
  error = error * .75 * x,
  y = 2 + 3 * x + error
)

sim_df = 
  bind_rows(const = sim_df_const, nonconst = sim_df_nonconst, .id = "data_source") 

sim_df %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm") +
  facet_grid(~data_source) 
```

**Notes: ** can see for the non-constant variance graph that the spread of data points start to increase toward the right tail. This won't effect the estimate, but it will impact the associated CIs (as seen below).

```{r}
lm(y ~ x, data = sim_df_const) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

lm(y ~ x, data = sim_df_nonconst) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

## **Bootstrapping Example**
-> Write a function to draw a bootstrap sample based on the dataframe.

```{r}
boot_sample = function(df) {
  sample_frac(df, size = 1, replace = TRUE)
}
```

```{r}
boot_sample(df = sim_df_nonconst) %>% 
  ggplot(aes(x=x, y=y)) +
  geom_point(alpha=0.5)
```
Note: A darker dot indicates that that observation was sampled repeatedly.

### Wanting to analyze a bunch of bootstrapped samples...
```{r}
boot_straps = 
  tibble(
    strap_num = 1:1000,
    strap_sample = rerun(1000, boot_sample(df=sim_df_nonconst))
  )
```
Note: each bootstrap sample is different from one another...

### Conducting analysis
```{r}
bootstrap_results = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(y ~ x, data = .x)), 
    results = map(models, broom::tidy)
  ) %>% 
  select(-strap_sample, -models) %>% 
  unnest
```

#### Graphing and comparing the multiple bootstrapped models...
```{r}
bootstrap_results %>% 
  group_by(term) %>%
  summarize(se = sd(estimate))
```


## Try the modelr package (pretty much does the exact same thing)
```{r}
sim_df_nonconst %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(y ~ x, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(boot_se = sd(estimate))
```


## What if your assumptions aren't wrong?
```{r}
sim_df_const %>% 
  lm(y~x, data = .) %>% 
  broom::tidy()

sim_df_const %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(boot_se = sd(estimate))
```

Note: If your data does meet the assumptions, then using bootstrapping should cause the variance to match!















